\section{Discussion}
The deviation percantage from the optimal or best known solutions in table \ref{table:table4} are important to be discussed first. They show if the algorithms can come up with schedules that are somewhat reasonable compared to the known optimal makespans without activity preemption. When these values become too high the comparison between the two algotithms becomes less valuable. This is because existing approaches without preemption can come up with better schedules already.
Looking at the algorithms produced for this research the results for the DC1 and J30 datasets are close enough to compare the approaches further. The percantage deviation from the non-preemptive makespans are within 3\% for the J30 and around 0\% for the DC1 dataset. This deviation is close enough to continue the comparison between the algorithms. The RG30 dataset on the other hand shows to be much harder to solve with deviations going up to 12\%. This could be a result of the high variance in serial/parallel indicator of the network structure for the RG30 dataset.

As for the motivation to allow preemption the percentage of tasks that could be improved in table \ref{table:table2} and by how much these instances are improved in table \ref{table:table3} the statement made in earlier research could be confirmed. In a non-insignificant amount of cases a reduction of makespan can be found when allowing preemption \cite{RN1}. The research on this topic can provide valuable insights when the absolute lowest schedule makespan is required.

The SAT algorithm performance in table \ref{table:table5} and the comparison of the SAT algorithm against the heuristic algorithm \ref{table:table6} gives the most important insight in the value and shortcomings of a SAT solver approach. For both the DC1 and J30 datasets the SAT solver matched or even outperformed the heuristic approach in around 90\% of the instances where it could find a solution in time. Finding a solution in time is also the biggest downside of using an approach aimed at finding the most optimal solution. For the DC1 dataset over 75\% was solved and this is better in most cases. For the J30 dataset with less than 30\% of instances getting a result withing time it is harder to decide what the better approach is and it depends much more on the practical application. When having the absolute minimum makespan is worth the risk of not having a solution or depending on a heuristic approach the SAT solver might still be worth it. In the cases it does provide a solution over 80\% is also proven to be optimal. This provides the certainty that no further effort has to be spend on trying to reduce the makespan.

Lastly a note on the impact of the setup times. The percantage of instances for which the makespan can be reduced when allowing preemption drops as the setup time increase as expected as shown in table \ref{table:table2}. The makespan deviations for improved instances show to much variance to draw any conclusions. Earlier research already showed the difference in setup times has a much lower impact on the average makespan reduction than allowing for preemption in the first place \cite{RN1}.